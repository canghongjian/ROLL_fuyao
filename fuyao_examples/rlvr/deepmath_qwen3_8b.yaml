defaults:
  - ../../examples/config/traj_envs@_here_
  - ../../examples/config/deepspeed_zero@_here_
  - ../../examples/config/deepspeed_zero2@_here_
  - ../../examples/config/deepspeed_zero3@_here_
  - ../../examples/config/deepspeed_zero3_cpuoffload@_here_

hydra:
  run:
    dir: .
  output_subdir: null

pretrain: /publicdata/huggingface.co/Qwen/Qwen3-8B
reward_pretrain: /publicdata/huggingface.co/Qwen/Qwen3-8B

exp_name: ${oc.env:BIFROST_JOB_NAME}_qwen3-8B-rlvr-deepmath
seed: 42
local_save_path: /log/roll_output
logging_dir: ${local_save_path}/logs
output_dir: ${local_save_path}
system_envs:
  USE_MODELSCOPE: '1'


checkpoint_config:
  type: file_system
  output_dir: ${local_save_path}/${exp_name}

track_with: swanlab  
tracker_kwargs:  
  login_kwargs:  
    api_key: ${oc.env:SWANLAB_API_KEY}  # 您的SwanLab API密钥  
  project: roll-experiments  # 项目名称  
  logdir: ${local_save_path}/swanlog  # 日志存储目录  
  experiment_name: ${exp_name}  # 实验名称  
  tags:  # 实验标签  
    - roll  
    - rl  
    - experiment

#track_with: wandb
#tracker_kwargs:
#  api_key: ${oc.env:WANDB_API_KEY}
#  project: roll-agentic
#  name: ${exp_name}_frozen_lake
#  notes: "agentic_pipeline"
#  tags:
#    - agentic
#    - roll
#    - baseline

# track_with: tensorboard
# tracker_kwargs:
#   log_dir: ${local_save_path}/tensorboard/roll_exp/agentic_frozen_lake

num_gpus_per_node: 8

max_steps: 500
save_steps: 100
logging_steps: 1
eval_steps: 20
resume_from_checkpoint: false


rollout_batch_size: 64  # prompt
prompt_length: 2048
response_length: 8192

num_return_sequences_in_group: 8
ppo_epochs: 1
adv_estimator: "grpo"
use_kl_loss: true
kl_loss_coef: 0.001
loss_agg_mode: "seq-mean-token-mean"

# clip
value_clip: 0.5
reward_clip: 10
advantage_clip: 2.0
dual_clip_loss: true

# normalize
norm_mean_type: ~
norm_std_type: ~


# reward
add_token_level_kl: false

# advantage
whiten_advantages: true

# dynamic sampling scheduler
# use_additional_prompts: true
# max_running_requests: 256
# is_num_return_sequences_expand: false


validation:
  data_args:
    template: qwen3
    file_name:
      - data/math_benchmarks.jsonl
  generating_args:
    max_new_tokens: ${response_length}
    top_p: 0.6
    top_k: 50
    num_beams: 1
    temperature: 0.6
    num_return_sequences: 1


actor_train:
  model_args:
    disable_gradient_checkpointing: false
    dtype: bf16
    model_type: ~
  training_args:
    learning_rate: 1.0e-6
    weight_decay: 0
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 32
    warmup_steps: 20
    num_train_epochs: 50
  data_args:
    template: qwen3
    file_name:
      - /root/workspace/zhangjh37@xiaopeng.com/tmp/deepmath_math_rule_20k.jsonl
    domain_interleave_probs:
      math_rule: 1.0
    messages: messages
    interleave_probs: "1.0"
    preprocessing_num_workers: 16
  strategy_args:
    strategy_name: megatron_train
    strategy_config:
      tensor_model_parallel_size: 2
      pipeline_model_parallel_size: 1
      expert_model_parallel_size: 1
      use_distributed_optimizer: true
      recompute_granularity: full
  device_mapping: list(range(0,8))
  infer_batch_size: 4

actor_infer:
  model_args:
    disable_gradient_checkpointing: true
    dtype: bf16
  generating_args:
    max_new_tokens: ${response_length}
    top_p: 0.99
    top_k: 100
    num_beams: 1
    temperature: 0.99
    num_return_sequences: ${num_return_sequences_in_group}
  data_args:
    template: qwen3
  strategy_args:
    strategy_name: sglang
    strategy_config:
      mem_fraction_static: 0.7
      load_format: dummy
  num_gpus_per_worker: 2
  device_mapping: list(range(0,8))
  infer_batch_size: 1

reference:
  model_args:
    disable_gradient_checkpointing: true
    dtype: bf16
    model_type: ~
  data_args:
    template: qwen3
  strategy_args:
    strategy_name: megatron_infer
    strategy_config:
      tensor_model_parallel_size: 1
      pipeline_model_parallel_size: 1
      expert_model_parallel_size: 1
  device_mapping: list(range(0,8))
  infer_batch_size: 2

rewards:
  math_rule:
    worker_cls: roll.pipeline.rlvr.rewards.math_rule_reward_worker.MathRuleRewardWorker
    model_args:
      model_name_or_path: ${reward_pretrain}
    data_args:
      template: qwen3
    tag_included: [deepmath_math_rule_20k]
    world_size: 8
    infer_batch_size: 1